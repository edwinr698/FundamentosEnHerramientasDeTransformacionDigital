{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AprendizajeProfundo_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaguzman/GENERAL-FundamentosEnHerramientasDeTransformacionDigital/blob/master/Unidad%203%20-%20Aprendizaje%20de%20m%C3%A1quinas/5_Aprendizaje%20Profundo/AprendizajeProfundo_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbj3-2Dvlhn",
        "colab_type": "text"
      },
      "source": [
        "# APRENDIZAJE PROFUNDO\n",
        "\n",
        "**Datos Bancarios de Marketing**\n",
        "\n",
        "El marketing es un componente clave de todas las empresas modernas. Las empresas reinvierten continuamente en marketing, tratando de identificar clientes potenciales. Sin embargo, el costo del marketing puede ser muy alto, lo que significa que la decisión del grupo objetivo es de gran importancia financiera.\n",
        "\n",
        "Este conjunto de datos fue dispuesto por una empresa minorista que recopiló datos históricos sobre sus clientes divididos en grupos. La empresa realizó un seguimiento de la rentabilidad de cada grupo individual después de una campaña de marketing y evaluó el gasto y retorno de la inversión de marketing para cada grupo.\n",
        "\n",
        "Información del dataset: https://www.kaggle.com/tsiaras/predicting-profitable-customer-segments?select=customerTargeting.csv\n",
        "\n",
        "\n",
        "**OBJETIVO**\n",
        "\n",
        "Crear un modelo de clasificación, basado en redes neuronales, que prediga cuál de los 2 grupos resultará más rentable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Pn9T77vn5E",
        "colab_type": "text"
      },
      "source": [
        "**Contenido**\n",
        "\n",
        "*   Importar librerías\n",
        "*   Base de datos\n",
        "*   Descripción y analítica de datos\n",
        "*   Preparación de los datos para el algoritmo\n",
        "*   Redes Neuronales: Clasificador\n",
        "*   Análisis de desempeño\n",
        "*   Realizar una predicción\n",
        "*   Actividad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MMaRwtivrPW",
        "colab_type": "text"
      },
      "source": [
        "# **LIBRERÍAS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_bWrw7DONvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "da58fa55-692a-4479-b121-5a4995b02a1a"
      },
      "source": [
        "# Importar las librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Librerías para graficar\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pylab import rcParams\n",
        "import graphviz\n",
        "\n",
        "# Librerías de Aprendizaje de máquinas\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical \n",
        "\n",
        "# Librerías para evaluar los clasificadores\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyT5WKixvtyE",
        "colab_type": "text"
      },
      "source": [
        "# **BASE DE DATOS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZoajo5NOnsv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "bcefab8a-c3dd-4a1b-ddb4-b3434a16c3f8"
      },
      "source": [
        "# import data\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/jaguzman/dataSets/master/datasets/customerTargeting.csv\",\n",
        "    sep = ',',         # separador de campos\n",
        "    thousands = None,  # separador de miles para números\n",
        "    decimal = '.')     # separador de los decimales para números\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>g1_1</th>\n",
              "      <th>g1_2</th>\n",
              "      <th>g1_3</th>\n",
              "      <th>g1_4</th>\n",
              "      <th>g1_5</th>\n",
              "      <th>g1_6</th>\n",
              "      <th>g1_7</th>\n",
              "      <th>g1_8</th>\n",
              "      <th>g1_9</th>\n",
              "      <th>g1_10</th>\n",
              "      <th>g1_11</th>\n",
              "      <th>g1_12</th>\n",
              "      <th>g1_13</th>\n",
              "      <th>g1_14</th>\n",
              "      <th>g1_15</th>\n",
              "      <th>g1_16</th>\n",
              "      <th>g1_17</th>\n",
              "      <th>g1_18</th>\n",
              "      <th>g1_19</th>\n",
              "      <th>g1_20</th>\n",
              "      <th>g1_21</th>\n",
              "      <th>g2_1</th>\n",
              "      <th>g2_2</th>\n",
              "      <th>g2_3</th>\n",
              "      <th>g2_4</th>\n",
              "      <th>g2_5</th>\n",
              "      <th>g2_6</th>\n",
              "      <th>g2_7</th>\n",
              "      <th>g2_8</th>\n",
              "      <th>g2_9</th>\n",
              "      <th>g2_10</th>\n",
              "      <th>g2_11</th>\n",
              "      <th>g2_12</th>\n",
              "      <th>g2_13</th>\n",
              "      <th>g2_14</th>\n",
              "      <th>g2_15</th>\n",
              "      <th>g2_16</th>\n",
              "      <th>g2_17</th>\n",
              "      <th>g2_18</th>\n",
              "      <th>g2_19</th>\n",
              "      <th>g2_20</th>\n",
              "      <th>g2_21</th>\n",
              "      <th>c_1</th>\n",
              "      <th>c_2</th>\n",
              "      <th>c_3</th>\n",
              "      <th>c_4</th>\n",
              "      <th>c_5</th>\n",
              "      <th>c_6</th>\n",
              "      <th>c_7</th>\n",
              "      <th>c_8</th>\n",
              "      <th>c_9</th>\n",
              "      <th>c_10</th>\n",
              "      <th>c_11</th>\n",
              "      <th>c_12</th>\n",
              "      <th>c_13</th>\n",
              "      <th>c_14</th>\n",
              "      <th>c_15</th>\n",
              "      <th>c_16</th>\n",
              "      <th>c_17</th>\n",
              "      <th>c_18</th>\n",
              "      <th>c_19</th>\n",
              "      <th>c_20</th>\n",
              "      <th>c_21</th>\n",
              "      <th>c_22</th>\n",
              "      <th>c_23</th>\n",
              "      <th>c_24</th>\n",
              "      <th>c_25</th>\n",
              "      <th>c_26</th>\n",
              "      <th>c_27</th>\n",
              "      <th>c_28</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.50</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>-2</td>\n",
              "      <td>2.505032</td>\n",
              "      <td>2.551406</td>\n",
              "      <td>6.240000</td>\n",
              "      <td>3.608000</td>\n",
              "      <td>0.744000</td>\n",
              "      <td>1.216000</td>\n",
              "      <td>0.003078</td>\n",
              "      <td>0.003028</td>\n",
              "      <td>0.578205</td>\n",
              "      <td>1.83</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>3</td>\n",
              "      <td>2.888736</td>\n",
              "      <td>2.616855</td>\n",
              "      <td>5.552000</td>\n",
              "      <td>0.728000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.002994</td>\n",
              "      <td>0.002953</td>\n",
              "      <td>0.586149</td>\n",
              "      <td>3.50</td>\n",
              "      <td>1.97</td>\n",
              "      <td>-1</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.223605</td>\n",
              "      <td>1</td>\n",
              "      <td>-3</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-6</td>\n",
              "      <td>-5</td>\n",
              "      <td>-0.383704</td>\n",
              "      <td>-0.065449</td>\n",
              "      <td>0.584000</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.232000</td>\n",
              "      <td>-1.944000</td>\n",
              "      <td>-0.007944</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.20</td>\n",
              "      <td>24</td>\n",
              "      <td>22</td>\n",
              "      <td>46</td>\n",
              "      <td>10</td>\n",
              "      <td>24</td>\n",
              "      <td>28</td>\n",
              "      <td>18</td>\n",
              "      <td>22</td>\n",
              "      <td>-4</td>\n",
              "      <td>-4</td>\n",
              "      <td>-8</td>\n",
              "      <td>3.718983</td>\n",
              "      <td>3.882271</td>\n",
              "      <td>7.423435</td>\n",
              "      <td>5.048030</td>\n",
              "      <td>0.836178</td>\n",
              "      <td>1.975244</td>\n",
              "      <td>0.784882</td>\n",
              "      <td>0.019448</td>\n",
              "      <td>0.680013</td>\n",
              "      <td>2.80</td>\n",
              "      <td>34</td>\n",
              "      <td>14</td>\n",
              "      <td>48</td>\n",
              "      <td>10</td>\n",
              "      <td>25</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>-8</td>\n",
              "      <td>1</td>\n",
              "      <td>4.065822</td>\n",
              "      <td>4.042015</td>\n",
              "      <td>6.369385</td>\n",
              "      <td>1.511704</td>\n",
              "      <td>1.783791</td>\n",
              "      <td>0.784882</td>\n",
              "      <td>0.033373</td>\n",
              "      <td>0.498949</td>\n",
              "      <td>3.25</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.541039</td>\n",
              "      <td>10</td>\n",
              "      <td>-12</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>-3</td>\n",
              "      <td>4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-9</td>\n",
              "      <td>-0.346839</td>\n",
              "      <td>-0.159744</td>\n",
              "      <td>-0.947614</td>\n",
              "      <td>0.463540</td>\n",
              "      <td>0</td>\n",
              "      <td>-5.342174</td>\n",
              "      <td>-1.321355</td>\n",
              "      <td>0.181064</td>\n",
              "      <td>1.85</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.00</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>-3</td>\n",
              "      <td>-8</td>\n",
              "      <td>-11</td>\n",
              "      <td>2.244550</td>\n",
              "      <td>2.458087</td>\n",
              "      <td>11.091399</td>\n",
              "      <td>5.853005</td>\n",
              "      <td>0.730046</td>\n",
              "      <td>2.022004</td>\n",
              "      <td>0.043937</td>\n",
              "      <td>0.014264</td>\n",
              "      <td>0.527707</td>\n",
              "      <td>1.30</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>25</td>\n",
              "      <td>4.918483</td>\n",
              "      <td>4.050389</td>\n",
              "      <td>10.029408</td>\n",
              "      <td>2.489174</td>\n",
              "      <td>0.204741</td>\n",
              "      <td>0.022247</td>\n",
              "      <td>0.042004</td>\n",
              "      <td>0.567984</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.70</td>\n",
              "      <td>-5</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.049024</td>\n",
              "      <td>-11</td>\n",
              "      <td>-7</td>\n",
              "      <td>-18</td>\n",
              "      <td>7</td>\n",
              "      <td>-5</td>\n",
              "      <td>-1</td>\n",
              "      <td>-3</td>\n",
              "      <td>-18</td>\n",
              "      <td>-18</td>\n",
              "      <td>-36</td>\n",
              "      <td>-2.673934</td>\n",
              "      <td>-1.592303</td>\n",
              "      <td>0.525305</td>\n",
              "      <td>-0.467169</td>\n",
              "      <td>0</td>\n",
              "      <td>-6.566521</td>\n",
              "      <td>-4.176403</td>\n",
              "      <td>-0.040277</td>\n",
              "      <td>2.05</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.91</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>-1</td>\n",
              "      <td>-3</td>\n",
              "      <td>-4</td>\n",
              "      <td>2.580190</td>\n",
              "      <td>2.683092</td>\n",
              "      <td>9.864426</td>\n",
              "      <td>2.582357</td>\n",
              "      <td>0.656638</td>\n",
              "      <td>1.407549</td>\n",
              "      <td>0.041563</td>\n",
              "      <td>0.021386</td>\n",
              "      <td>0.261785</td>\n",
              "      <td>4.50</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>17</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>16</td>\n",
              "      <td>-4</td>\n",
              "      <td>-9</td>\n",
              "      <td>-13</td>\n",
              "      <td>1.964163</td>\n",
              "      <td>2.278147</td>\n",
              "      <td>3.369489</td>\n",
              "      <td>0.665585</td>\n",
              "      <td>2.163561</td>\n",
              "      <td>0.043937</td>\n",
              "      <td>0.010358</td>\n",
              "      <td>0.273886</td>\n",
              "      <td>3.60</td>\n",
              "      <td>1.98</td>\n",
              "      <td>-1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.284503</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>-10</td>\n",
              "      <td>0</td>\n",
              "      <td>-3</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0.616027</td>\n",
              "      <td>0.404945</td>\n",
              "      <td>-1.506923</td>\n",
              "      <td>0.741964</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.438120</td>\n",
              "      <td>-0.787132</td>\n",
              "      <td>-0.012101</td>\n",
              "      <td>1.82</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.50</td>\n",
              "      <td>23</td>\n",
              "      <td>16</td>\n",
              "      <td>39</td>\n",
              "      <td>14</td>\n",
              "      <td>33</td>\n",
              "      <td>25</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>8</td>\n",
              "      <td>-9</td>\n",
              "      <td>-1</td>\n",
              "      <td>3.470617</td>\n",
              "      <td>3.055989</td>\n",
              "      <td>11.672962</td>\n",
              "      <td>4.554560</td>\n",
              "      <td>1.895740</td>\n",
              "      <td>1.237122</td>\n",
              "      <td>0.941241</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.390180</td>\n",
              "      <td>3.00</td>\n",
              "      <td>29</td>\n",
              "      <td>23</td>\n",
              "      <td>52</td>\n",
              "      <td>8</td>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>21</td>\n",
              "      <td>23</td>\n",
              "      <td>9</td>\n",
              "      <td>-2</td>\n",
              "      <td>7</td>\n",
              "      <td>4.527831</td>\n",
              "      <td>4.215284</td>\n",
              "      <td>4.494986</td>\n",
              "      <td>1.419174</td>\n",
              "      <td>1.144728</td>\n",
              "      <td>0.364776</td>\n",
              "      <td>0.008148</td>\n",
              "      <td>0.347568</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1.80</td>\n",
              "      <td>-3</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.648418</td>\n",
              "      <td>0</td>\n",
              "      <td>-13</td>\n",
              "      <td>-13</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>-4</td>\n",
              "      <td>-4</td>\n",
              "      <td>10</td>\n",
              "      <td>-18</td>\n",
              "      <td>-8</td>\n",
              "      <td>-1.057214</td>\n",
              "      <td>-1.159294</td>\n",
              "      <td>0.751012</td>\n",
              "      <td>-0.182052</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.259728</td>\n",
              "      <td>0.059574</td>\n",
              "      <td>0.042613</td>\n",
              "      <td>1.99</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    g1_1  g1_2  g1_3  g1_4  g1_5  ...      c_25      c_26      c_27  c_28  target\n",
              "0   4.50     1     3     4     5  ... -3.232000 -1.944000 -0.007944  1.76       2\n",
              "1   2.20    24    22    46    10  ... -5.342174 -1.321355  0.181064  1.85       1\n",
              "2  12.00     7     4    11    18  ... -6.566521 -4.176403 -0.040277  2.05       2\n",
              "3   1.91     8     5    13    14  ... -2.438120 -0.787132 -0.012101  1.82       0\n",
              "4   2.50    23    16    39    14  ... -1.259728  0.059574  0.042613  1.99       2\n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTxa1yQevxhQ",
        "colab_type": "text"
      },
      "source": [
        "**DESCRIPCIÓN DE LA BASE DE DATOS**\n",
        "\n",
        "**La base de datos está estructurada de la siguiente manera**\n",
        "\n",
        "Cada fila es una comparación entre dos grupos de clientes potenciales:\n",
        "\n",
        "1. Los nombres de columna que comienzan con **\"g1\"** representan las características del primer grupo de clientes (se conocían antes de que se ejecutara la campaña).\n",
        "\n",
        "2. Los nombres de columna que comienzan con **\"g2\"** representan las características del segundo grupo de clientes (se conocían antes de que la campaña fuera correr).\n",
        "\n",
        "3. Los nombres de columna que comienzan con **\"c_\"** son características que representan alguna comparación de los dos grupos (también conocidos antes de que se ejecutara la campaña).\n",
        "\n",
        "**Variable de salida**\n",
        "\n",
        "La última columna, llamada \"target\", es categórica, con 3 categorías:\n",
        "\n",
        "**[0]:** ninguno de los dos grupos fue rentable.\n",
        "\n",
        "**[1]:** grupo1 resultó ser más rentable.\n",
        "\n",
        "**[2]:** grupo2 resultó ser más rentable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7YhS52ov1DK",
        "colab_type": "text"
      },
      "source": [
        "# **DESCRIPCIÓN Y ANALÍTICA DE DATOS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr8s0FeiVhpa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26782992-6a69-4aec-a325-cb7fe3354e9f"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6620 entries, 0 to 6619\n",
            "Data columns (total 71 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   g1_1    6620 non-null   float64\n",
            " 1   g1_2    6620 non-null   int64  \n",
            " 2   g1_3    6620 non-null   int64  \n",
            " 3   g1_4    6620 non-null   int64  \n",
            " 4   g1_5    6620 non-null   int64  \n",
            " 5   g1_6    6620 non-null   int64  \n",
            " 6   g1_7    6620 non-null   int64  \n",
            " 7   g1_8    6620 non-null   int64  \n",
            " 8   g1_9    6620 non-null   int64  \n",
            " 9   g1_10   6620 non-null   int64  \n",
            " 10  g1_11   6620 non-null   int64  \n",
            " 11  g1_12   6620 non-null   int64  \n",
            " 12  g1_13   6620 non-null   float64\n",
            " 13  g1_14   6620 non-null   float64\n",
            " 14  g1_15   6620 non-null   float64\n",
            " 15  g1_16   6620 non-null   float64\n",
            " 16  g1_17   6620 non-null   float64\n",
            " 17  g1_18   6620 non-null   float64\n",
            " 18  g1_19   6620 non-null   float64\n",
            " 19  g1_20   6620 non-null   float64\n",
            " 20  g1_21   6620 non-null   float64\n",
            " 21  g2_1    6620 non-null   float64\n",
            " 22  g2_2    6620 non-null   int64  \n",
            " 23  g2_3    6620 non-null   int64  \n",
            " 24  g2_4    6620 non-null   int64  \n",
            " 25  g2_5    6620 non-null   int64  \n",
            " 26  g2_6    6620 non-null   int64  \n",
            " 27  g2_7    6620 non-null   int64  \n",
            " 28  g2_8    6620 non-null   int64  \n",
            " 29  g2_9    6620 non-null   int64  \n",
            " 30  g2_10   6620 non-null   int64  \n",
            " 31  g2_11   6620 non-null   int64  \n",
            " 32  g2_12   6620 non-null   int64  \n",
            " 33  g2_13   6620 non-null   float64\n",
            " 34  g2_14   6620 non-null   float64\n",
            " 35  g2_15   6620 non-null   float64\n",
            " 36  g2_16   6620 non-null   float64\n",
            " 37  g2_17   6620 non-null   float64\n",
            " 38  g2_18   6620 non-null   float64\n",
            " 39  g2_19   6620 non-null   float64\n",
            " 40  g2_20   6620 non-null   float64\n",
            " 41  g2_21   6620 non-null   float64\n",
            " 42  c_1     6620 non-null   float64\n",
            " 43  c_2     6620 non-null   int64  \n",
            " 44  c_3     6620 non-null   int64  \n",
            " 45  c_4     6620 non-null   int64  \n",
            " 46  c_5     6620 non-null   int64  \n",
            " 47  c_6     6620 non-null   int64  \n",
            " 48  c_7     6620 non-null   int64  \n",
            " 49  c_8     6620 non-null   int64  \n",
            " 50  c_9     6620 non-null   float64\n",
            " 51  c_10    6620 non-null   int64  \n",
            " 52  c_11    6620 non-null   int64  \n",
            " 53  c_12    6620 non-null   int64  \n",
            " 54  c_13    6620 non-null   int64  \n",
            " 55  c_14    6620 non-null   int64  \n",
            " 56  c_15    6620 non-null   int64  \n",
            " 57  c_16    6620 non-null   int64  \n",
            " 58  c_17    6620 non-null   int64  \n",
            " 59  c_18    6620 non-null   int64  \n",
            " 60  c_19    6620 non-null   int64  \n",
            " 61  c_20    6620 non-null   float64\n",
            " 62  c_21    6620 non-null   float64\n",
            " 63  c_22    6620 non-null   float64\n",
            " 64  c_23    6620 non-null   float64\n",
            " 65  c_24    6620 non-null   int64  \n",
            " 66  c_25    6620 non-null   float64\n",
            " 67  c_26    6620 non-null   float64\n",
            " 68  c_27    6620 non-null   float64\n",
            " 69  c_28    6620 non-null   float64\n",
            " 70  target  6620 non-null   int64  \n",
            "dtypes: float64(30), int64(41)\n",
            "memory usage: 3.6 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONTrUoU51a_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "c210001d-e723-472a-ea2a-a1bd69eefdbc"
      },
      "source": [
        "#Graficar la cantidad de grupos con respecto a su resultado\n",
        "\n",
        "# Esta función realiza una suma de cada categoría y luego las grafica \n",
        "sns.countplot(data['target']) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcaf0ef3320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASRElEQVR4nO3df6zd9X3f8eeLX2FZaIFxy4jt1SjzOjlZYxKPsDJNKVHAoHambRKBluBlrM406BKpqkSiaaTpmDKtSZRkCZI73JgqC6NNMtyIjXkMLWpUAteUAjZl3BIYtgi+xQRIo9CZvvfH+dz21Fz7c2x8zvH1fT6ko/v9vr+f7/f7vrqYl74/T6oKSZIO56RpNyBJOv4ZFpKkLsNCktRlWEiSugwLSVLXKdNuYBzOOeecWr169bTbkKQlZefOnX9SVTOLLTshw2L16tXMzs5Ouw1JWlKSPHWoZZ6GkiR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdZ2QT3Br+fi/n/h7027hhPe3/s3D025BxwGPLCRJXWMLiySnJ7kvyR8m2ZXkV1v9/CTfTjKX5L8kOa3VX9fm59ry1UPb+mirP5bksnH1LEla3DiPLF4GLqmqtwLrgA1JLgL+PfCZqvrbwPPAtW38tcDzrf6ZNo4ka4GrgDcDG4AvJjl5jH1Lkg4ytrCoge+32VPbp4BLgN9p9W3AlW16Y5unLX9XkrT6bVX1clV9B5gDLhxX35KkVxvrNYskJyd5ENgH7AD+GPheVR1oQ/YAK9r0CuBpgLb8BeBvDNcXWWd4X5uTzCaZnZ+fH8evI0nL1ljDoqpeqap1wEoGRwN/d4z72lJV66tq/czMot/dIUk6ShO5G6qqvgfcA/wD4MwkC7fsrgT2tum9wCqAtvxHgeeG64usI0magHHeDTWT5Mw2/deAdwOPMgiN97Rhm4A72vT2Nk9b/r+qqlr9qna31PnAGuC+cfUtSXq1cT6Udx6wrd25dBJwe1V9I8lu4LYk/xb4A+CWNv4W4LeSzAH7GdwBRVXtSnI7sBs4AFxXVa+MsW9J0kHGFhZV9RBwwSL1J1jkbqaq+iHw3kNs6ybgpmPdoyRpND7BLUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUtfYwiLJqiT3JNmdZFeSD7f6x5PsTfJg+1wxtM5Hk8wleSzJZUP1Da02l+SGcfUsSVrcKWPc9gHgl6vqgSRnADuT7GjLPlNVvz48OMla4CrgzcAbgf+Z5O+0xV8A3g3sAe5Psr2qdo+xd0nSkLGFRVU9AzzTpl9K8iiw4jCrbARuq6qXge8kmQMubMvmquoJgCS3tbGGhSRNyESuWSRZDVwAfLuVrk/yUJKtSc5qtRXA00Or7Wm1Q9UP3sfmJLNJZufn54/xbyBJy9vYwyLJG4CvAh+pqheBm4E3AesYHHl86ljsp6q2VNX6qlo/MzNzLDYpSWrGec2CJKcyCIovV9XXAKrq2aHlvwF8o83uBVYNrb6y1ThMXZI0AeO8GyrALcCjVfXpofp5Q8N+DnikTW8HrkryuiTnA2uA+4D7gTVJzk9yGoOL4NvH1bck6dXGeWRxMfAB4OEkD7bax4Crk6wDCngS+BBAVe1KcjuDC9cHgOuq6hWAJNcDdwEnA1uratcY+5YkHWScd0P9HpBFFt15mHVuAm5apH7n4daTJI2XT3BLkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkrrGFhZJViW5J8nuJLuSfLjVz06yI8nj7edZrZ4kn0syl+ShJG8b2tamNv7xJJvG1bMkaXHjPLI4APxyVa0FLgKuS7IWuAG4u6rWAHe3eYDLgTXtsxm4GQbhAtwIvAO4ELhxIWAkSZMxtrCoqmeq6oE2/RLwKLAC2Ahsa8O2AVe26Y3ArTVwL3BmkvOAy4AdVbW/qp4HdgAbxtW3JOnVJnLNIslq4ALg28C5VfVMW/Rd4Nw2vQJ4emi1Pa12qPrB+9icZDbJ7Pz8/DHtX5KWu7GHRZI3AF8FPlJVLw4vq6oC6ljsp6q2VNX6qlo/MzNzLDYpSWrGGhZJTmUQFF+uqq+18rPt9BLt575W3wusGlp9Zasdqi5JmpBx3g0V4Bbg0ar69NCi7cDCHU2bgDuG6te0u6IuAl5op6vuAi5Ncla7sH1pq0mSJuSUMW77YuADwMNJHmy1jwGfBG5Pci3wFPC+tuxO4ApgDvgB8EGAqtqf5NeA+9u4T1TV/jH2LUk6yNjCoqp+D8ghFr9rkfEFXHeIbW0Fth677iRJR8InuCVJXYaFJKlrnNcsJOmwLv78xdNu4YT3rV/61jHZjkcWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV0jhUWSu0epSZJOTId9ziLJ6cDrgXPaS/wWXt/xIyzynRKSpBNT76G8DwEfAd4I7OQvw+JF4D+OsS9J0nHksGFRVZ8FPpvkl6rq8xPqSZJ0nBnpdR9V9fkkPwWsHl6nqm4dU1+SpOPISGGR5LeANwEPAq+0cgGGhSQtA6O+SHA9sLZ954QkaZkZ9TmLR4C/Oc5GJEnHr1GPLM4Bdie5D3h5oVhV/3gsXU3Y23/Fs2njtvM/XDPtFiS9BqOGxcfH2YQk6fg26t1Q/3vcjUiSjl+j3g31EoO7nwBOA04F/rSqfmRcjUmSjh+jHlmcsTCdJMBG4KJxNSVJOr4c8Vtna+C/ApeNoR9J0nFo1NNQPz80exKD5y5+OJaOJEnHnVHvhvrZoekDwJMMTkVJkpaBUa9ZfPBIN5xkK/AzwL6qekurfRz4RWC+DftYVd3Zln0UuJbB60T+VVXd1eobgM8CJwP/qao+eaS9SJJem1G//Ghlkq8n2dc+X02ysrPal4ANi9Q/U1Xr2mchKNYCVwFvbut8McnJSU4GvgBcDqwFrm5jJUkTNOoF7t8EtjP4Xos3Ar/baodUVd8E9o+4/Y3AbVX1clV9B5gDLmyfuap6oqr+DLgNT39J0sSNGhYzVfWbVXWgfb4EzBzlPq9P8lCSre3b92DwrXtPD43Z02qHqr9Kks1JZpPMzs/PLzZEknSURg2L55K8f+HUUJL3A88dxf5uZvCq83XAM8CnjmIbi6qqLVW1vqrWz8wcbY5JkhYzalj8M+B9wHcZ/E/+PcA/PdKdVdWzVfVKVf058BsMTjMB7AVWDQ1d2WqHqkuSJmjUsPgEsKmqZqrqxxiEx68e6c6SnDc0+3MMXn0Og+shVyV5XZLzgTXAfcD9wJok5yc5jcFF8O1Hul9J0msz6nMWP1lVzy/MVNX+JBccboUkXwHeCZyTZA9wI/DOJOsYvGfqSeBDbXu7ktwO7GbwHMd1VfVK2871wF0Mbp3dWlW7Rv/1JEnHwqhhcVKSsxYCI8nZvXWr6upFyrccZvxNwE2L1O8E7hyxT0nSGIwaFp8Cfj/Jb7f597LI/9glSSemUZ/gvjXJLHBJK/18Ve0eX1uSpOPJqEcWtHAwICRpGTriV5RLkpYfw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoaW1gk2ZpkX5JHhmpnJ9mR5PH286xWT5LPJZlL8lCStw2ts6mNfzzJpnH1K0k6tHEeWXwJ2HBQ7Qbg7qpaA9zd5gEuB9a0z2bgZhiEC3Aj8A7gQuDGhYCRJE3O2MKiqr4J7D+ovBHY1qa3AVcO1W+tgXuBM5OcB1wG7Kiq/VX1PLCDVweQJGnMJn3N4tyqeqZNfxc4t02vAJ4eGren1Q5Vf5Ukm5PMJpmdn58/tl1L0jI3tQvcVVVAHcPtbamq9VW1fmZm5lhtVpLE5MPi2XZ6ifZzX6vvBVYNjVvZaoeqS5ImaNJhsR1YuKNpE3DHUP2adlfURcAL7XTVXcClSc5qF7YvbTVJ0gSdMq4NJ/kK8E7gnCR7GNzV9Eng9iTXAk8B72vD7wSuAOaAHwAfBKiq/Ul+Dbi/jftEVR180VySNGZjC4uquvoQi961yNgCrjvEdrYCW49ha5KkI+QT3JKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkrqmERZInkzyc5MEks612dpIdSR5vP89q9ST5XJK5JA8leds0epak5WyaRxY/XVXrqmp9m78BuLuq1gB3t3mAy4E17bMZuHninUrSMnc8nYbaCGxr09uAK4fqt9bAvcCZSc6bRoOStFxNKywK+B9JdibZ3GrnVtUzbfq7wLltegXw9NC6e1rtr0iyOclsktn5+flx9S1Jy9IpU9rvP6yqvUl+DNiR5I+GF1ZVJakj2WBVbQG2AKxfv/6I1pUkHd5Ujiyqam/7uQ/4OnAh8OzC6aX2c18bvhdYNbT6ylaTJE3IxMMiyV9PcsbCNHAp8AiwHdjUhm0C7mjT24Fr2l1RFwEvDJ2ukiRNwDROQ50LfD3Jwv7/c1X99yT3A7cnuRZ4CnhfG38ncAUwB/wA+ODkW5ak5W3iYVFVTwBvXaT+HPCuReoFXDeB1iRJh3A83TorSTpOGRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1LZmwSLIhyWNJ5pLcMO1+JGk5WRJhkeRk4AvA5cBa4Ooka6fblSQtH0siLIALgbmqeqKq/gy4Ddg45Z4kadlIVU27h64k7wE2VNU/b/MfAN5RVdcPjdkMbG6zPwE8NvFGJ+cc4E+m3YSOmn+/petE/9v9eFXNLLbglEl3Mi5VtQXYMu0+JiHJbFWtn3YfOjr+/Zau5fy3WyqnofYCq4bmV7aaJGkClkpY3A+sSXJ+ktOAq4DtU+5JkpaNJXEaqqoOJLkeuAs4GdhaVbum3NY0LYvTbScw/35L17L92y2JC9ySpOlaKqehJElTZFhIkroMiyXG154sXUm2JtmX5JFp96Ijk2RVknuS7E6yK8mHp93TpHnNYglprz35P8C7gT0M7hK7uqp2T7UxjSTJPwK+D9xaVW+Zdj8aXZLzgPOq6oEkZwA7gSuX0789jyyWFl97soRV1TeB/dPuQ0euqp6pqgfa9EvAo8CK6XY1WYbF0rICeHpofg/L7D9YadqSrAYuAL493U4my7CQpBEleQPwVeAjVfXitPuZJMNiafG1J9KUJDmVQVB8uaq+Nu1+Js2wWFp87Yk0BUkC3AI8WlWfnnY/02BYLCFVdQBYeO3Jo8Dty/y1J0tKkq8Avw/8RJI9Sa6ddk8a2cXAB4BLkjzYPldMu6lJ8tZZSVKXRxaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLKSjkOTMJP9yAvu5Msnace9H6jEspKNzJjByWGTgaP69XQkYFpo6n7OQjkKShTf+PgbcA/wkcBZwKvCvq+qO9sK5uxi8cO7twBXANcD7gXkGL4XcWVW/nuRNwBeAGeAHwC8CZwPfAF5on1+oqj+e0K8o/RWnTLsBaYm6AXhLVa1Lcgrw+qp6Mck5wL1JFl7DsgbYVFX3Jvn7wC8Ab2UQKg8w+F4EgC3Av6iqx5O8A/hiVV3StvONqvqdSf5y0sEMC+m1C/Dv2pcb/TmD18af25Y9VVX3tumLgTuq6ofAD5P8LvzFm0x/CvjtwSuIAHjdpJqXRmFYSK/dP2Fw+ujtVfX/kjwJnN6W/ekI658EfK+q1o2pP+k18wK3dHReAs5o0z8K7GtB8dPAjx9inW8BP5vk9HY08TMA7XsRvpPkvfAXF8Pfush+pKkxLKSjUFXPAd9K8giwDlif5GEGF7D/6BDr3M/glfIPAf8NeJjBhWsYHJ1cm+QPgV385dfl3gb8SpI/aBfBpanwbihpgpK8oaq+n+T1wDeBzQvf7Swdz7xmIU3WlvaQ3enANoNCS4VHFpKkLq9ZSJK6DAtJUpdhIUnqMiwkSV2GhSSp6/8DrPv/dWIqqykAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsKEtrPLwFqo",
        "colab_type": "text"
      },
      "source": [
        "# **PREPARACIÓN DE LOS DATOS PARA EL ALGORITMO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-CALe0KPPp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "30703acc-8d3c-4bb9-9cd9-3376de863a22"
      },
      "source": [
        "# Se normalizan los datos\n",
        "\n",
        "target_column = ['target'] \n",
        "predictors = list(set(list(data.columns))-set(target_column))\n",
        "data[predictors] = data[predictors]/data[predictors].max()\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>g1_1</th>\n",
              "      <th>g1_2</th>\n",
              "      <th>g1_3</th>\n",
              "      <th>g1_4</th>\n",
              "      <th>g1_5</th>\n",
              "      <th>g1_6</th>\n",
              "      <th>g1_7</th>\n",
              "      <th>g1_8</th>\n",
              "      <th>g1_9</th>\n",
              "      <th>g1_10</th>\n",
              "      <th>g1_11</th>\n",
              "      <th>g1_12</th>\n",
              "      <th>g1_13</th>\n",
              "      <th>g1_14</th>\n",
              "      <th>g1_15</th>\n",
              "      <th>g1_16</th>\n",
              "      <th>g1_17</th>\n",
              "      <th>g1_18</th>\n",
              "      <th>g1_19</th>\n",
              "      <th>g1_20</th>\n",
              "      <th>g1_21</th>\n",
              "      <th>g2_1</th>\n",
              "      <th>g2_2</th>\n",
              "      <th>g2_3</th>\n",
              "      <th>g2_4</th>\n",
              "      <th>g2_5</th>\n",
              "      <th>g2_6</th>\n",
              "      <th>g2_7</th>\n",
              "      <th>g2_8</th>\n",
              "      <th>g2_9</th>\n",
              "      <th>g2_10</th>\n",
              "      <th>g2_11</th>\n",
              "      <th>g2_12</th>\n",
              "      <th>g2_13</th>\n",
              "      <th>g2_14</th>\n",
              "      <th>g2_15</th>\n",
              "      <th>g2_16</th>\n",
              "      <th>g2_17</th>\n",
              "      <th>g2_18</th>\n",
              "      <th>g2_19</th>\n",
              "      <th>g2_20</th>\n",
              "      <th>g2_21</th>\n",
              "      <th>c_1</th>\n",
              "      <th>c_2</th>\n",
              "      <th>c_3</th>\n",
              "      <th>c_4</th>\n",
              "      <th>c_5</th>\n",
              "      <th>c_6</th>\n",
              "      <th>c_7</th>\n",
              "      <th>c_8</th>\n",
              "      <th>c_9</th>\n",
              "      <th>c_10</th>\n",
              "      <th>c_11</th>\n",
              "      <th>c_12</th>\n",
              "      <th>c_13</th>\n",
              "      <th>c_14</th>\n",
              "      <th>c_15</th>\n",
              "      <th>c_16</th>\n",
              "      <th>c_17</th>\n",
              "      <th>c_18</th>\n",
              "      <th>c_19</th>\n",
              "      <th>c_20</th>\n",
              "      <th>c_21</th>\n",
              "      <th>c_22</th>\n",
              "      <th>c_23</th>\n",
              "      <th>c_24</th>\n",
              "      <th>c_25</th>\n",
              "      <th>c_26</th>\n",
              "      <th>c_27</th>\n",
              "      <th>c_28</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "      <td>6620.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.117773</td>\n",
              "      <td>0.277385</td>\n",
              "      <td>0.223099</td>\n",
              "      <td>0.264996</td>\n",
              "      <td>0.549403</td>\n",
              "      <td>0.219870</td>\n",
              "      <td>0.236303</td>\n",
              "      <td>0.223864</td>\n",
              "      <td>0.272481</td>\n",
              "      <td>0.067730</td>\n",
              "      <td>-0.110438</td>\n",
              "      <td>-0.002270</td>\n",
              "      <td>0.630829</td>\n",
              "      <td>0.609380</td>\n",
              "      <td>0.500860</td>\n",
              "      <td>0.378314</td>\n",
              "      <td>0.301282</td>\n",
              "      <td>0.364064</td>\n",
              "      <td>0.205070</td>\n",
              "      <td>0.058852</td>\n",
              "      <td>0.449405</td>\n",
              "      <td>0.117314</td>\n",
              "      <td>0.274798</td>\n",
              "      <td>0.213161</td>\n",
              "      <td>0.259098</td>\n",
              "      <td>0.551254</td>\n",
              "      <td>0.230174</td>\n",
              "      <td>0.247299</td>\n",
              "      <td>0.227852</td>\n",
              "      <td>0.270009</td>\n",
              "      <td>0.072485</td>\n",
              "      <td>-0.104746</td>\n",
              "      <td>0.002047</td>\n",
              "      <td>0.636691</td>\n",
              "      <td>0.610591</td>\n",
              "      <td>0.360480</td>\n",
              "      <td>0.309278</td>\n",
              "      <td>0.352006</td>\n",
              "      <td>0.205567</td>\n",
              "      <td>0.058207</td>\n",
              "      <td>0.448996</td>\n",
              "      <td>0.205229</td>\n",
              "      <td>0.649430</td>\n",
              "      <td>0.120277</td>\n",
              "      <td>0.119905</td>\n",
              "      <td>0.173464</td>\n",
              "      <td>0.183686</td>\n",
              "      <td>0.200906</td>\n",
              "      <td>0.183686</td>\n",
              "      <td>0.200906</td>\n",
              "      <td>0.364757</td>\n",
              "      <td>0.107450</td>\n",
              "      <td>-0.140250</td>\n",
              "      <td>-0.003011</td>\n",
              "      <td>-0.002325</td>\n",
              "      <td>-0.004013</td>\n",
              "      <td>0.003283</td>\n",
              "      <td>0.003567</td>\n",
              "      <td>0.086642</td>\n",
              "      <td>-0.162628</td>\n",
              "      <td>-0.003076</td>\n",
              "      <td>-0.006080</td>\n",
              "      <td>-0.001529</td>\n",
              "      <td>-0.001388</td>\n",
              "      <td>0.003118</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018183</td>\n",
              "      <td>-0.012635</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.442756</td>\n",
              "      <td>1.031722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.080771</td>\n",
              "      <td>0.205784</td>\n",
              "      <td>0.178387</td>\n",
              "      <td>0.193351</td>\n",
              "      <td>0.281799</td>\n",
              "      <td>0.165410</td>\n",
              "      <td>0.174303</td>\n",
              "      <td>0.165922</td>\n",
              "      <td>0.189745</td>\n",
              "      <td>0.176692</td>\n",
              "      <td>0.276798</td>\n",
              "      <td>0.197846</td>\n",
              "      <td>0.186245</td>\n",
              "      <td>0.165308</td>\n",
              "      <td>0.183420</td>\n",
              "      <td>0.169903</td>\n",
              "      <td>0.156013</td>\n",
              "      <td>0.177895</td>\n",
              "      <td>0.273416</td>\n",
              "      <td>0.151767</td>\n",
              "      <td>0.139392</td>\n",
              "      <td>0.096028</td>\n",
              "      <td>0.197035</td>\n",
              "      <td>0.175566</td>\n",
              "      <td>0.187533</td>\n",
              "      <td>0.283348</td>\n",
              "      <td>0.167302</td>\n",
              "      <td>0.175899</td>\n",
              "      <td>0.174043</td>\n",
              "      <td>0.194449</td>\n",
              "      <td>0.186179</td>\n",
              "      <td>0.268173</td>\n",
              "      <td>0.192640</td>\n",
              "      <td>0.185767</td>\n",
              "      <td>0.164932</td>\n",
              "      <td>0.160174</td>\n",
              "      <td>0.158099</td>\n",
              "      <td>0.172940</td>\n",
              "      <td>0.273798</td>\n",
              "      <td>0.151470</td>\n",
              "      <td>0.139194</td>\n",
              "      <td>0.057535</td>\n",
              "      <td>0.078034</td>\n",
              "      <td>0.312578</td>\n",
              "      <td>0.312109</td>\n",
              "      <td>0.218859</td>\n",
              "      <td>0.387257</td>\n",
              "      <td>0.400708</td>\n",
              "      <td>0.387257</td>\n",
              "      <td>0.400708</td>\n",
              "      <td>0.228176</td>\n",
              "      <td>0.207250</td>\n",
              "      <td>0.275574</td>\n",
              "      <td>0.195550</td>\n",
              "      <td>0.196491</td>\n",
              "      <td>0.277324</td>\n",
              "      <td>0.186564</td>\n",
              "      <td>0.206131</td>\n",
              "      <td>0.162345</td>\n",
              "      <td>0.308895</td>\n",
              "      <td>0.199056</td>\n",
              "      <td>0.253208</td>\n",
              "      <td>0.242978</td>\n",
              "      <td>0.222032</td>\n",
              "      <td>0.241445</td>\n",
              "      <td>0.161435</td>\n",
              "      <td>0.269918</td>\n",
              "      <td>0.236848</td>\n",
              "      <td>0.139141</td>\n",
              "      <td>0.069786</td>\n",
              "      <td>0.731042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.045652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.562500</td>\n",
              "      <td>-1.225806</td>\n",
              "      <td>-0.855263</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034613</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.574468</td>\n",
              "      <td>-1.161290</td>\n",
              "      <td>-0.807692</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043257</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.131579</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.769231</td>\n",
              "      <td>-0.769231</td>\n",
              "      <td>-0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.682927</td>\n",
              "      <td>-1.424242</td>\n",
              "      <td>-0.851351</td>\n",
              "      <td>-0.868421</td>\n",
              "      <td>-1.260870</td>\n",
              "      <td>-0.823529</td>\n",
              "      <td>-1.111111</td>\n",
              "      <td>-0.520000</td>\n",
              "      <td>-1.761905</td>\n",
              "      <td>-0.935185</td>\n",
              "      <td>-0.971578</td>\n",
              "      <td>-0.982609</td>\n",
              "      <td>-0.841247</td>\n",
              "      <td>-1.101848</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.210149</td>\n",
              "      <td>-1.118416</td>\n",
              "      <td>-1.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.072478</td>\n",
              "      <td>0.115385</td>\n",
              "      <td>0.085106</td>\n",
              "      <td>0.106383</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.081967</td>\n",
              "      <td>0.093023</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.115385</td>\n",
              "      <td>-0.041667</td>\n",
              "      <td>-0.258065</td>\n",
              "      <td>-0.108553</td>\n",
              "      <td>0.499821</td>\n",
              "      <td>0.499283</td>\n",
              "      <td>0.415205</td>\n",
              "      <td>0.270721</td>\n",
              "      <td>0.197695</td>\n",
              "      <td>0.244902</td>\n",
              "      <td>0.011054</td>\n",
              "      <td>0.001827</td>\n",
              "      <td>0.348935</td>\n",
              "      <td>0.060976</td>\n",
              "      <td>0.109091</td>\n",
              "      <td>0.063830</td>\n",
              "      <td>0.113402</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.098361</td>\n",
              "      <td>0.093023</td>\n",
              "      <td>0.088889</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>-0.042553</td>\n",
              "      <td>-0.258065</td>\n",
              "      <td>-0.102564</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500441</td>\n",
              "      <td>0.258457</td>\n",
              "      <td>0.204211</td>\n",
              "      <td>0.237271</td>\n",
              "      <td>0.011054</td>\n",
              "      <td>0.001827</td>\n",
              "      <td>0.347931</td>\n",
              "      <td>0.171053</td>\n",
              "      <td>0.597938</td>\n",
              "      <td>-0.076923</td>\n",
              "      <td>-0.076923</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168967</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.272727</td>\n",
              "      <td>-0.094595</td>\n",
              "      <td>-0.105263</td>\n",
              "      <td>-0.130435</td>\n",
              "      <td>-0.088235</td>\n",
              "      <td>-0.083333</td>\n",
              "      <td>-0.013333</td>\n",
              "      <td>-0.309524</td>\n",
              "      <td>-0.092593</td>\n",
              "      <td>-0.148597</td>\n",
              "      <td>-0.120835</td>\n",
              "      <td>-0.147435</td>\n",
              "      <td>-0.143721</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.176891</td>\n",
              "      <td>-0.157556</td>\n",
              "      <td>-0.081497</td>\n",
              "      <td>0.394919</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.093478</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.191489</td>\n",
              "      <td>0.234043</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.196721</td>\n",
              "      <td>0.209302</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.020833</td>\n",
              "      <td>-0.064516</td>\n",
              "      <td>-0.013158</td>\n",
              "      <td>0.581047</td>\n",
              "      <td>0.553559</td>\n",
              "      <td>0.514066</td>\n",
              "      <td>0.373449</td>\n",
              "      <td>0.289963</td>\n",
              "      <td>0.363012</td>\n",
              "      <td>0.065102</td>\n",
              "      <td>0.006409</td>\n",
              "      <td>0.482790</td>\n",
              "      <td>0.085366</td>\n",
              "      <td>0.254545</td>\n",
              "      <td>0.170213</td>\n",
              "      <td>0.226804</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.213115</td>\n",
              "      <td>0.232558</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>-0.064516</td>\n",
              "      <td>-0.012821</td>\n",
              "      <td>0.586219</td>\n",
              "      <td>0.554388</td>\n",
              "      <td>0.356669</td>\n",
              "      <td>0.298497</td>\n",
              "      <td>0.352255</td>\n",
              "      <td>0.067168</td>\n",
              "      <td>0.006314</td>\n",
              "      <td>0.483239</td>\n",
              "      <td>0.184211</td>\n",
              "      <td>0.652921</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.377317</td>\n",
              "      <td>0.073171</td>\n",
              "      <td>-0.090909</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053333</td>\n",
              "      <td>-0.095238</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.009503</td>\n",
              "      <td>-0.001521</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.427252</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.121739</td>\n",
              "      <td>0.403846</td>\n",
              "      <td>0.319149</td>\n",
              "      <td>0.382979</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.327869</td>\n",
              "      <td>0.348837</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.403846</td>\n",
              "      <td>0.145833</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.065789</td>\n",
              "      <td>0.751262</td>\n",
              "      <td>0.698992</td>\n",
              "      <td>0.609732</td>\n",
              "      <td>0.484974</td>\n",
              "      <td>0.395679</td>\n",
              "      <td>0.481274</td>\n",
              "      <td>0.314664</td>\n",
              "      <td>0.029305</td>\n",
              "      <td>0.554830</td>\n",
              "      <td>0.134146</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.319149</td>\n",
              "      <td>0.371134</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.327869</td>\n",
              "      <td>0.372093</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.148936</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.755853</td>\n",
              "      <td>0.700167</td>\n",
              "      <td>0.463317</td>\n",
              "      <td>0.403157</td>\n",
              "      <td>0.463541</td>\n",
              "      <td>0.314664</td>\n",
              "      <td>0.028546</td>\n",
              "      <td>0.553774</td>\n",
              "      <td>0.210526</td>\n",
              "      <td>0.701031</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.559174</td>\n",
              "      <td>0.219512</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.081081</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.088235</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.173333</td>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.092593</td>\n",
              "      <td>0.135783</td>\n",
              "      <td>0.111573</td>\n",
              "      <td>0.137561</td>\n",
              "      <td>0.159016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.144024</td>\n",
              "      <td>0.126101</td>\n",
              "      <td>0.082238</td>\n",
              "      <td>0.466513</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              g1_1         g1_2  ...         c_28       target\n",
              "count  6620.000000  6620.000000  ...  6620.000000  6620.000000\n",
              "mean      0.117773     0.277385  ...     0.442756     1.031722\n",
              "std       0.080771     0.205784  ...     0.069786     0.731042\n",
              "min       0.045652     0.000000  ...     0.000000     0.000000\n",
              "25%       0.072478     0.115385  ...     0.394919     0.000000\n",
              "50%       0.093478     0.250000  ...     0.427252     1.000000\n",
              "75%       0.121739     0.403846  ...     0.466513     2.000000\n",
              "max       1.000000     1.000000  ...     1.000000     2.000000\n",
              "\n",
              "[8 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzpv6RQ417Nl",
        "colab_type": "text"
      },
      "source": [
        "Se separa la base de datos entre datos de entrenamiento y datos de pruebas (80% - 20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNb6Jn7cPm18",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "169b9a24-3785-4486-dd50-a9b42cacae45"
      },
      "source": [
        "X = data[predictors].values\n",
        "y = data[target_column].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)\n",
        "print(X_train.shape); print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5296, 70)\n",
            "(1324, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3663F7Rn2JDE",
        "colab_type": "text"
      },
      "source": [
        "**Pasar de variables categórica a variable numéricas**\n",
        "\n",
        "En este caso se realiza una representación en 3 columnas, uno para cada clase. Para la clase a la que pertenece a una fila, se marca con un 1 en la columna correspondinete, en los demás se coloca un 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6kt3G_FWoMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffee8d62-2b4e-48ea-cb2b-f331f4df04c8"
      },
      "source": [
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "# Se cuentas en número de clases que han en la base de datos\n",
        "count_classes = y_train.shape[1]\n",
        "print(count_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9sfnU6FwJOz",
        "colab_type": "text"
      },
      "source": [
        "# **REDES NEURONALES: CLASIFICADOR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZcOVC-YX5y8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "1d7a8a25-4d93-4eca-f244-b5522da3d3de"
      },
      "source": [
        "# Se inicia el modelo\n",
        "model = Sequential()\n",
        "\n",
        "# Se agrega la primer capa con 140 neuronas, función de activación 'relu' y el vector de entrada es de 70 (70 características)\n",
        "model.add(Dense(140, activation='relu', input_shape=(70,)))\n",
        "\n",
        "# Se agrega una capa oculta con 280 neuronas y función de activación 'relu'\n",
        "model.add(Dense(280, activation='relu'))\n",
        "\n",
        "# Se agrega una capa oculta con 140 neuronas y función de activación 'relu'\n",
        "model.add(Dense(140, activation='relu'))\n",
        "\n",
        "# Se agrega la capa final con 3 neuronas (3 clases), función de activación 'sigmoid'\n",
        "model.add(Dense(3, activation='sigmoid'))\n",
        "\n",
        "# Se compila el modelo\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Se entrena el modelos\n",
        "print(X_train.shape, y_train.shape)\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=100, verbose=1)\n",
        "# batch_size: número de muestras que serán propagadas por la red\n",
        "# batch_size: muestra el progreso en el entrenamiento de la red neuronal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5296, 70) (5296, 3)\n",
            "Epoch 1/20\n",
            "5296/5296 [==============================] - 0s 66us/step - loss: 0.5800 - accuracy: 0.7041\n",
            "Epoch 2/20\n",
            "5296/5296 [==============================] - 0s 40us/step - loss: 0.5485 - accuracy: 0.7327\n",
            "Epoch 3/20\n",
            "5296/5296 [==============================] - 0s 40us/step - loss: 0.5402 - accuracy: 0.7390\n",
            "Epoch 4/20\n",
            "5296/5296 [==============================] - 0s 40us/step - loss: 0.5355 - accuracy: 0.7394\n",
            "Epoch 5/20\n",
            "5296/5296 [==============================] - 0s 39us/step - loss: 0.5328 - accuracy: 0.7437\n",
            "Epoch 6/20\n",
            "5296/5296 [==============================] - 0s 38us/step - loss: 0.5309 - accuracy: 0.7419\n",
            "Epoch 7/20\n",
            "5296/5296 [==============================] - 0s 48us/step - loss: 0.5294 - accuracy: 0.7428\n",
            "Epoch 8/20\n",
            "5296/5296 [==============================] - 0s 37us/step - loss: 0.5248 - accuracy: 0.7483\n",
            "Epoch 9/20\n",
            "5296/5296 [==============================] - 0s 39us/step - loss: 0.5205 - accuracy: 0.7491\n",
            "Epoch 10/20\n",
            "5296/5296 [==============================] - 0s 38us/step - loss: 0.5199 - accuracy: 0.7488\n",
            "Epoch 11/20\n",
            "5296/5296 [==============================] - 0s 37us/step - loss: 0.5157 - accuracy: 0.7518\n",
            "Epoch 12/20\n",
            "5296/5296 [==============================] - 0s 38us/step - loss: 0.5137 - accuracy: 0.7531\n",
            "Epoch 13/20\n",
            "5296/5296 [==============================] - 0s 37us/step - loss: 0.5072 - accuracy: 0.7569\n",
            "Epoch 14/20\n",
            "5296/5296 [==============================] - 0s 38us/step - loss: 0.5039 - accuracy: 0.7583\n",
            "Epoch 15/20\n",
            "5296/5296 [==============================] - 0s 39us/step - loss: 0.4985 - accuracy: 0.7631\n",
            "Epoch 16/20\n",
            "5296/5296 [==============================] - 0s 38us/step - loss: 0.4906 - accuracy: 0.7628\n",
            "Epoch 17/20\n",
            "5296/5296 [==============================] - 0s 37us/step - loss: 0.4876 - accuracy: 0.7663\n",
            "Epoch 18/20\n",
            "5296/5296 [==============================] - 0s 38us/step - loss: 0.4778 - accuracy: 0.7711\n",
            "Epoch 19/20\n",
            "5296/5296 [==============================] - 0s 39us/step - loss: 0.4717 - accuracy: 0.7749\n",
            "Epoch 20/20\n",
            "5296/5296 [==============================] - 0s 39us/step - loss: 0.4637 - accuracy: 0.7784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fcaef7877b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYWa8dP-wYWT",
        "colab_type": "text"
      },
      "source": [
        "# **ANÁLISIS DE DESEMPEÑO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6oNJq46WyRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "93801f82-54b3-46f8-d9a9-53ba763ce8f0"
      },
      "source": [
        "# Imprimir la exactitud del modelo en el entrenamiento y en la prueba\n",
        "pred_train= model.predict(X_train)\n",
        "scores = model.evaluate(X_train, y_train, verbose=0)\n",
        "print('Exactitud en el entrenamiento: ', scores[1])   \n",
        " \n",
        "pred_test= model.predict(X_test)\n",
        "scores2 = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Exactitud en la prueba: ', scores2[1])   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exactitud en el entrenamiento:  0.7815332412719727\n",
            "Exactitud en la prueba:  0.7356495261192322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qtp4z_Qwhx5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d1af62c9-1fc0-4fe1-f4c5-7f77df64d0b8"
      },
      "source": [
        "# Matriz de confusión para el modelo de Árbol de Decisión\n",
        "\n",
        "# Predicción del modelo\n",
        "y_pred = model.predict(X_test)\n",
        "# Matriz de confusión\n",
        "cm = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "# Nombres de las clases\n",
        "target_names = ['ninguno', 'g1 rentable', 'g2 rentable']\n",
        "# Propiedades de la gráfica\n",
        "ax= plt.subplot()\n",
        "# Imprimir la matriz de confusión\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", ax = ax, cmap=\"YlGnBu\"); #annot=True to annotate cells\n",
        "# Nombre del eje x\n",
        "ax.set_xlabel('Predicción');\n",
        "# Nombre del eje y\n",
        "ax.set_ylabel('Clases'); \n",
        "# Título de la gráfica\n",
        "ax.set_title('Matriz de Confusión'); \n",
        "# Nombres de las clases en eje x\n",
        "ax.xaxis.set_ticklabels(target_names); \n",
        "# Nombres de las clases en eje y\n",
        "ax.yaxis.set_ticklabels(target_names);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcqXLM4Uzw-o",
        "colab_type": "text"
      },
      "source": [
        "# **PREDICCIÓN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ltcwU2TV0Bf",
        "colab_type": "text"
      },
      "source": [
        "Si se tienen dos grupos nuevos los siguientes datos. ¿A qué clase pertenecen estos 2 grupos?\n",
        "\n",
        "**[0]:** ninguno de los dos grupos es rentable.\n",
        "\n",
        "**[1]:** grupo1 es más rentable.\n",
        "\n",
        "**[2]:** grupo2 es más rentable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJrPPPQdVgQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newG =  np.array ([[ 0.11111111,  0.05613476,  0.1147541 , -0.10666667,  0.18604651,\n",
        "         0.5535025 ,  0.52794587,  0.27378263, -0.03947368,  1.        ,\n",
        "         0.13829787,  0.27906977,  0.        ,  0.08333333,  0.54416965,\n",
        "         0.42974303,  0.06203501,  0.04918033,  0.52848822,  0.16666667,\n",
        "        -0.02564103,  0.02879231,  0.14126488,  0.09615385,  0.06060606,\n",
        "         0.01793659, -0.19512195,  0.        ,  0.54358497,  0.36870884,\n",
        "         0.0702439 ,  0.13877945,  0.06451613,  0.60195296, -0.01740262,\n",
        "         0.25      ,  0.09677419,  0.35      ,  0.44665573,  0.10869565,\n",
        "         0.62118674, -0.15789474,  0.        , -0.17391304,  0.17368421,\n",
        "         0.17021277, -0.08108108,  0.10909091,  0.        , -0.01728713,\n",
        "        -0.15384615,  0.021068  ,  0.95      ,  0.33495702, -0.07675274,\n",
        "         0.52497478,  0.19587629, -0.10638298,  0.27659574,  0.        ,\n",
        "         0.26666667,  0.00759701, -0.00925926,  0.6975945 ,  0.30769231,\n",
        "         0.39722864,  0.19230769,  0.18      , -0.10416667,  0.05146746]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VEvGWWIVvT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31b4c3a2-811d-4dfb-c825-883e6133ed0f"
      },
      "source": [
        "# Se realiza la predicción\n",
        "prediction = model.predict(newG)\n",
        "# Se muestra la predicción\n",
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5033915 , 0.45842794, 0.08122335]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3lsBhlHZ1JT",
        "colab_type": "text"
      },
      "source": [
        "La predicción del modelo dice que ninguno de los dos grupos es rentable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro07U9j1z0b0",
        "colab_type": "text"
      },
      "source": [
        "# **ACTIVIDAD**\n",
        "\n",
        "1. Crear un modelo de Redes Neuronales cambiando la cantidad neuronas por capa.\n",
        "2. Crear un modelo de Redes Neuronales agregando otra capa oculta de neuronas.\n",
        "3. Determine cual de los dos modelos es el mejor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmVyoqkcajXq",
        "colab_type": "text"
      },
      "source": [
        "**Solución 1. modelo de Redes Neuronales cambiando la cantidad neuronas por capa**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKLN1ZUMaqpn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "6fd5d984-402c-4d79-e75f-3c6ff28dacd0"
      },
      "source": [
        "# Se inicia el modelo\n",
        "model1 = Sequential()\n",
        "\n",
        "# Se agrega la primer capa con 100 neuronas, función de activación 'relu' y el vector de entrada es de 70 (70 características)\n",
        "model1.add(Dense(__, activation='__', input_shape=(70,)))\n",
        "\n",
        "# Se agrega una capa oculta con 100 neuronas y función de activación 'relu'\n",
        "model1.add(Dense(__, activation='__'))\n",
        "\n",
        "# Se agrega una capa oculta con 100 neuronas y función de activación 'relu'\n",
        "model1.add(Dense(__, activation='__'))\n",
        "\n",
        "# Se agrega la capa final con 3 neuronas (3 clases), función de activación 'sigmoid'\n",
        "model1.add(Dense(3, activation='sigmoid'))\n",
        "\n",
        "# Se compila el modelo\n",
        "model1.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Se entrena el modelos\n",
        "print(X_train.shape, y_train.shape)\n",
        "model1.fit(X_train, y_train, epochs=5, batch_size=1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5296, 70) (5296, 3)\n",
            "Epoch 1/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5631 - accuracy: 0.7225\n",
            "Epoch 2/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5496 - accuracy: 0.7319\n",
            "Epoch 3/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5457 - accuracy: 0.7346\n",
            "Epoch 4/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5434 - accuracy: 0.7385\n",
            "Epoch 5/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5408 - accuracy: 0.7395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fcae62c4780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-x512yfa2HW",
        "colab_type": "text"
      },
      "source": [
        "**Solución 2. Modelo de Redes Neuronales agregando otra capa oculta de neuronas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfpFSh-ha-aJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "321a81b4-922b-407b-8da3-a3988dcf67cf"
      },
      "source": [
        "# Se inicia el modelo\n",
        "model2 = Sequential()\n",
        "\n",
        "# Se agrega la primer capa con 100 neuronas, función de activación 'relu' y el vector de entrada es de 70 (70 características)\n",
        "model2.add(Dense(100, activation='relu', input_shape=(70,)))\n",
        "\n",
        "# Se agrega la capa final con 3 neuronas (3 clases), función de activación 'sigmoid'\n",
        "model2.add(Dense(3, activation='sigmoid'))\n",
        "\n",
        "# Se compila el modelo\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Se entrena el modelos\n",
        "print(X_train.shape, y_train.shape)\n",
        "model2.fit(X_train, y_train, epochs=5, batch_size=1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5296, 70) (5296, 3)\n",
            "Epoch 1/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5629 - accuracy: 0.7230\n",
            "Epoch 2/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5507 - accuracy: 0.7319\n",
            "Epoch 3/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5477 - accuracy: 0.7323\n",
            "Epoch 4/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5447 - accuracy: 0.7371\n",
            "Epoch 5/5\n",
            "5296/5296 [==============================] - 7s 1ms/step - loss: 0.5424 - accuracy: 0.7374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fcae61be470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z97QhLT1bFIu",
        "colab_type": "text"
      },
      "source": [
        "**Solución 3. Comparación de los modelos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKLHA_RsbN1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "39ed9f41-bdfd-44ba-f89a-91be0f92b473"
      },
      "source": [
        "# Imprimir la exactitud del modelo en el entrenamiento y en la prueba\n",
        " \n",
        "pred_test= __.predict(X_test)\n",
        "scores1 = __.evaluate(X_test, y_test, verbose=0)\n",
        "print('Exactitud Modelo 1: ', scores1[1])   \n",
        "\n",
        "pred_test= model2.predict(__)\n",
        "scores2 = model2.evaluate(__, __, verbose=0)\n",
        "print('Exactitud Modelo 2: ', scores2[1]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exactitud Modelo 1:  0.746223509311676\n",
            "Exactitud Modelo 2:  0.7452166080474854\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}